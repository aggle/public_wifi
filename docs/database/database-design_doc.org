#+TITLE: RDPy Survey Design Doc
#+ROAM_KEY: rdpy

* Steps
  1) [X] Defining a mission statement and mission objectives
  2) [X] Analyzing the current database
  3) [ ] Creating the data structures
  4) [ ] Determining and establishing table relationships
  5) [ ] Determining and defining business rules
  6) [ ] Determining and defining views
  7) [ ] Reviewing data integrity

* Mission statement
The purpose of the Trumpler 14 project is to determine the occurrence rate of wide-separation binaries in the Trumpler 14 young massive cluster, using PSF subtraction to access closer working angles. The purpose of the Trumpler 14 *database* is to provide the required information for, and track the progress of, the processing of thousands of point sources.

* Mission objectives
- Compile the initial catalog of point sources
  - Position, magnitude, and all instances of an astrophysical object
- Make sure each point source corresponds to 1, and only 1, astrophysical object
- Keep track of which astrophysical objects are included in the database
- Keep track of which point sources are in which stamp
- Keep track of which point sources come from which exposure
- Keep track of which point sources are associated with a particular astrophyiscal object
- Keep track of which stamps are suitable to use as reference PSFs
- Track stages of PSF subtraction:
  1) Unsubtracted
  2) Subtracted
  3) SNR'd
  4) Completeness'd
- Keep track of which stamps are used as references for all PSF subtractions
- Keep track of the KLIP parameters used for all PSF subtractions
- Keep track of which candidates appear under which PSF subtraction conditions
- Keep track of which primary a candidate is associated with
- Keep track of photometry and position for each candidate
- Keep track of photometric KLIP correction for each point source
- Keep track of photometric KLIP correction for each candidate
- Maintain completeness information for each stamp
- Track model-derived properties for primaries and companions
  - mass
  - mostly mass


* Analyzing the current database
  A description of all fields in the ONC database can be found [[./list_of_fields-strampelli.org][here]]
  The List of Fields can be found [[file:list_of_fields.org][here]]

* Create the data structures
  I'm creating the Final List of Tables [[list_of_tables.org][here]]
  
* Determine and establish table relationships

* Determine and define business rules
  
* Determine and define views

* Review data integrity

* Appendices
** Processing steps
  The database should be designed to support these analysis steps:
  1. Generate *point source* catalog
     - x, y *coordinates* of each point source
     - RA, Dec coordinates of each point source
     - *Photometry*
     - Map point sources to *catalog designations*
  2. Create *stamps*
     - Identify central pixel
     - choose stamp size around it
  3. Perform PSF subtraction on each stamp
     - Select *target*
     - Select appropriate *references* using *image similarity metrics*
     - Select appropriate *KL parameters*
     - Store *residual stamps*
  4. Search for *companions*  or *limits*
     - measure *residual noise*
     - define detection threshold
       - based on residual noise and number of observations of the primary
  5. Measure *completeness*
     - *Artificial star* injection and recovery
     - Compute AUC for ROC curves
  6. Assign *masses* or *mass limits* to primaries, companions, and non-detections

** Written out in long form 
*** Assemble stamp library
    The first step is to assemble all the *flt files* and extract the *point sources*. The *x and y   positions* of each point source should be recorded, and converted to *RA and Dec coordinates* using *WCS information* from the *fits headers*. Then, you can identify point sources that come from the same *astrophysical object* by matching their RA and Dec, as well as *magnitude* for point sources taken with the same *filter*. Using the RA, Dec, and magnitude, you can also match them to *catalog designations* e.g. from Gaia. You must also decide on the *membership* of the point source.
*** Prepare and perform PSF subtraction
    Now you can start setting up PSF subtraction. To start this process, you need to cut out a *stamp* around each point source. To pick a good stamp size, look at a histogram of *distances* between each pair of sources and pick a good stamp size where you balance having enough pixels for PSF subtraction without losing too many sources to contamination. You also need to check the stamps for *quality* - *hot pixels*, *saturation*, and maybe other things I haven't thought of. 
    Once you have a library of stamps, you can start figuring out which stamps are good *reference images* for each other. Due to distortion across the detector, it's best to use PSFs that are close to each other. Divide the detector into 100 *quadrants* and group together the stars that are within the same quadrant. You can also calculate *image similarity metrics* for PSFs within the same quadrant (or across the whole detector) to rank PSFs by similarity. Reference images must also be taken using the same *filter*.
    Finally, it's time to actually do PSF subtraction. Loop (serial or in parallel) over each stamp and perform a *KLIP subtraction*. Choose your *KLIP parameters* -- which in this case is just Kklip -- and construct your *reference image* from your chosen references. Then store the *psf-subtracted residual* for analysis. Stamps can be used as reference images as long as they do not contain point sources, extended structures, or other anomalies.
*** Analyze residuals
    Congratulations, you now have a version of each stamp with the primary PSF removed. What does this mean? There are two different things you need to find out:
    1. Are there any companions in the residual data?
    2. What is the brightest companion you /would/ have seen, if it /were/ there? Aka what is the faintest object for which you are X% complete (e.g. 95%)? You can also measure the *completeness* as a function of *separation* and *magnitude*. 
**** Detecting companions
     To detect a companion, you must test the *flux* in each location against some *threshold value*. This threshold can be computed using the remaining pixels in the image, or (I think) by comparing against the flux in the same pixel in the residual of the references, since these are known to not have companions. This threshold should also take into account multiple *visits* to the same object.
**** Characterizing companions
     A companion is characterized by its *flux*, as well as its *separation* and *position angle* relative to the primary.
**** Measuring completeness
     Completeness is measured by injecting *artificial stars* at different *contrasts* /before/ PSF subtraction, and measuring if the *residual flux* at the *injection location* is above the threshold or not. The end product is an *ROC curve*, from which you can compute the *Area Under the Curve (AUC)*. These values can be computed individually for each stamp with small-number statistics, or aggregated over all stamps. In this case, stamps should be binned together by magnitude, with each *magnitude bin* analyzed separately. 
*** ROC curve analysis
    Sensitivity range explored as a function of:
    - primary magnitude (10-22, in bins of 1)
    - *delta magnitude* (bins of 1)
    - separation (0"-1", steps of 0.1")
    - position angle (0-2pi)
**** Steps to build TPR and FPR:
     1. Create 1000 fake binaries - create a PSF model from KLIP, rescale it, and perturb it using the error map. Then inject a scaled companion (or do not inject a scaled companion) in the target pixel
     2. Perform PSF subtraction described above
     3. Measure SNR in target pixel and declare it above or below threshold for detection.
     4. Build ROC curves and compute AUC.

** Table format
   The tables will be stored as HDF5? files with the following fields:
- NAME: the name of the header (same as the filename, no suffix)
- DESCR: one-line description of the table contents (e.g. Primary Headers from the original data FITS files)
- TABLE: this field stores the actual table
Writing tables to file is handled by the table_utils.write_table() function.
table_utils.write_table() also creates (or updates) an entry in a file called list_of_tables.csv, which can be read by table_utils.list_tables() (or list_available_tables())

Maybe all the tables should be stored in one big HDF file, under different keys, so that they can be pulled form the file by name

** Tables
** RA and Dec tables: see [[file:~/Documents/org-notes/2020-05-01.org][2020-05-01 notes]]
   The drizzlepac function `pixtosky` will give you the RA and Dec for each pixel of a WFC3 image. Every image from the same subarray (or full frame) will have the same number of pixels, so they can be stored in the same table. I can't ssh into AZG right now to run it and see what the output looks like, but either way the values for a file should be indexed by a hierarchical index with two levels: file_id, and coord (RA or Dec). Then the pixel values can be stored raveled in a column (for a dataframe), or collapsed inside a single cell (series). 

   Update: so if you want to save it in a portable format like HDF5 or .csv, and not in a python-specific format like a pickle, then you'll have to do some real thinking about how to store this data. maybe the column is a raveled pixel coordinate? See [[file:2020-05-01.org][2020-05-02 notes]].

   
   
